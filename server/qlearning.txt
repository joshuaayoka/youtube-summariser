Q Learning
 Initialise random Q Value table, update iteratively using experiences gathered by agent

Bellman formula: ğ‘„(ğ‘ ,ğ‘)â†ğ‘„(ğ‘ ,ğ‘)+ğ›¼[ğ‘Ÿ+ğ›¾maxğ‘â€²ğ‘„(ğ‘ â€²,ğ‘â€²)âˆ’ğ‘„(ğ‘ ,ğ‘)]
Policy determines which action to take in each state and can be derived from Q-values. Exploitation: policy chooses action which highest Q-value. Exploration: sometimes less optimal action is chosen for exploration purposes.
Epsilon-greedy strategy: agent mostly chooses best-known action but occasionally tries random action.

Discount Factor (Î³): This factor discounts the value of future rewards compared to immediate rewards. A higher discount factor means that future rewards are more valuable, encouraging long-term beneficial actions over short-term gains.
